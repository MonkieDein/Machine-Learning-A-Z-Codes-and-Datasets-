revenue - cost - depreciation
revenue - cost - depreciation
rev - cost - depreciation
(rev - cost - depreciation)*(1-0.35) + depreciation
CF = (rev - cost - depreciation)*(1-0.35) + depreciation
CF/(1+r) +  CF/(1+r)^2 +  CF/(1+r)^3
CF/(1+r) +  CF/(1+r)^2 +  CF/(1+r)^3  -capital
23000/8
dep = (2000-500)/3
rev = 5000
var = 3000
fix = 800
T = 0.34
(rev-var-fix-dep)*(1-T)+dep
CF = (rev-var-fix-dep)*(1-T)+dep
r = 0.1
CF/(1+r)+CF/(1+r)^2+CF/(1+r)^3
CF/(1+r)+CF/(1+r)^2+CF/(1+r)^3 - 2000
CF/(1+r)+CF/(1+r)^2+CF/(1+r)^3 - 2000 + 500
shiny::runApp('C:/GITHUB/Rasr_submit/Rshiny/Risk Measure/Distribution')
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/R")
ave(dataset$Age,FUN = function(x) mean(x,na.rm=TRUE))
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/R")
# Importing the dataset
dataset = read.csv('Data.csv')
ave(dataset$Age,FUN = function(x) mean(x,na.rm=TRUE))
mean(dataset$Age,na.rm=TRUE)
Age
dataset$Age
ifelse(is.na(dataset),ave(dataset$Age,FUN = function(x) mean(x,na.rm=TRUE)),
dataset$Age)
ifelse(is.na(dataset$Age),ave(dataset$Age,FUN = function(x) mean(x,na.rm=TRUE)),
dataset$Age)
ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm=TRUE)),dataset$Age)
ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm=TRUE),dataset$Age)
ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm=TRUE),dataset$Age)
ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm=TRUE),dataset$Age)
dataset$Salary = ifelse(is.na(dataset$Salary),mean(dataset$Salary,na.rm=TRUE),dataset$Salary)
# Dealing with missing data
# 1) Replace missing data with the mean
dataset$Age = ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm=TRUE),dataset$Age)
dataset
dataset = read.csv('Data.csv')
# Dealing with missing data
# 1) Replace missing data with the mean
var_of_interest = c("Age","Salary")
for(v in var_of_interest){
dataset[[v]] = ifelse(is.na(dataset[[v]]),mean(dataset[[v]],na.rm=TRUE),dataset[[v]])
}
View(dataset)
mean(dataset[[v]],na.rm=TRUE)
dataset2
# Importing the dataset
dataset2 = read.csv('Data.csv')
dataset2
dataset2
is.na.data.frame(dataset2)
rowSums(is.na.data.frame(dataset2))
rowSums(!is.na.data.frame(dataset2))
!rowSums(is.na.data.frame(dataset2))
# 2) remove NA data
dataset2 = dataset2[!rowSums(is.na.data.frame(dataset2)),]
dataset2
dataset$Country
dataset2 = read.csv('Data.csv',stringsAsFactors = TRUE)
dataset2 = dataset2[!rowSums(is.na.data.frame(dataset2)),]
dataset2
dataset2$Country
dataset2$Purchased
# Splitting the dataset into (Training set) and (Test set)
install.packages("caTools")
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased,SplitRatio = 0.8)
Split
split
split = sample.split(dataset$Purchased,SplitRatio = 0.8)
training_set = subset(dataset,split == TRUE)
test_set = subset(dataset,split == FALSE)
sample(c("TRUE","FALSE"),replace=TRUE,prob=c(0.2,0.8))
sample(c("TRUE","FALSE"),replace=TRUE,prob=c(0.2,0.8))
sample(c("TRUE","FALSE"),size=nrow(dataset),replace=TRUE,prob=c(0.2,0.8))
sample(c(TRUE,FALSE),size=nrow(dataset),replace=TRUE,prob=c(0.2,0.8))
sample(c(TRUE,FALSE),size=nrow(dataset),replace=TRUE,prob=c(0.2,0.8))
sample(c(TRUE,FALSE),size=nrow(dataset),replace=TRUE,prob=c(0.2,0.8))
sample(c(TRUE,FALSE),size=nrow(dataset),replace=TRUE,prob=c(0.2,0.8))
training_set[c("Age","Salary")] = scale(training_setc("Age","Salary"))
)
test_set[c("Age","Salary")] = scale(test_set[c("Age","Salary")])
# Feature Scaling on numeric value
# 1) Standardisation : (x-mean(x))/std(x)
# 2) Normalization : x-min(x)/(max(x)-min(x))
# Most ML method are based on L2 distance :
# Variable with larger range dominate effect the function.
training_set[c("Age","Salary")] = scale(training_set[c("Age","Salary")])
test_set[c("Age","Salary")] = scale(test_set[c("Age","Salary")])
training_set
test_set
scale(1:10,scale=TRUE)
scale(1:10,scale=FALSE)
scale(1:10,center = FALSE,scale=apply(x, 2, sd, na.rm = TRUE))
scale(1:10,center = FALSE,scale=apply(1:10, 2, sd, na.rm = TRUE))
1:10/apply(1:10, 2, sd, na.rm = TRUE)
1:10/sd(1:10)
1:10/(10-1)
1:10/(10-1)
scale(1:10,scale=FALSE)
scale(1:10,scale=TRUE)
scale(1:10,center=FALSE,scale=TRUE)
1:10/(10-1)
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 4 - Simple Linear Regression/R")
# Importing the dataset where string variable is treated as factor
dataset = read.csv('Salary_Data.csv',stringsAsFactors = TRUE)
#
# Ordinary Least Square
# Simple Linear regression, given X predict Y as Yhat.
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 4 - Simple Linear Regression/R")
# Importing the dataset where string variable is treated as factor
df = read.csv('Salary_Data.csv',stringsAsFactors = TRUE)
# Dealing with missing data
# 1) Replace missing data with the mean
var_of_interest = c("Age","Salary")
is.na.dataset(df)
is.na.data.frame(df)
rowSums(is.na.data.frame(df))
library(caTools)
set.seed(123)
split = sample.split(df$Salary,SplitRatio = 2/3)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = Salary ~ YearsExperience,
data = train_df)
summary(regressor)
# Predicting the test results
y_pred = predict(regressor,newdata = test_df)
y_pred
test_df
# Data Visualization
library(ggplot2)
regressor$fitted.values
ggplot() + geom_point(aes(x = train_df$YearsExperience,y = train_df$Salary),color = 'red') +
geom_line(aes(x=train_df$YearsExperience,y = regressor$fitted.values),color = 'blue')
ggplot() + geom_point(aes(x = train_df$YearsExperience,y = train_df$Salary),color = 'red') +
geom_line(aes(x=train_df$YearsExperience,y = regressor$fitted.values),color = 'blue') +
ggtitle('Salary vs Experience (Training Set)') + xlab('Years of Experience') + ylab('Salary')
ggplot() + geom_point(aes(x = train_df$YearsExperience,y = train_df$Salary),color = 'red') +
geom_line(aes(x=train_df$YearsExperience,y = predict(regressor,newdata = train_df)),color = 'blue') +
ggtitle('Salary vs Experience (Training Set)') + xlab('Years of Experience') + ylab('Salary')
regressor
regressor$fitted.values
# Visualizing the Test Set Result
ggplot() + geom_point(aes(x = test_df$YearsExperience,y = test_df$Salary),color = 'red') +
geom_line(aes(x=test_df$YearsExperience,y = predict(regressor,newdata = test_df)),color = 'blue') +
ggtitle('Salary vs Experience (Test Set)') + xlab('Years of Experience') + ylab('Salary')
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression")
#
# Ordinary Least Square
# Simple Linear regression, given X predict Y as Yhat.
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression")
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")
df = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
df
# 2) Backward Elimination (remove highest P-value < 0.05)
# 3) Multivariate Normality (Adding the smallest P-value variable < 0.05)
# 4) Bidirectional (Alternate 2 and 3)
# 5) Score Comparison (eg: adjusted R-square)
#
# Multiple Linear regression, given X predict Y as Yhat.
# For non linear prediction one could use feature transformation on X or Y
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Salary,SplitRatio = 0.8)
split = sample.split(df$Profit,SplitRatio = 0.8)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = Profit ~ ., data = train_df)
regressor
# Summarize the regression output
summary(regressor)
# re-fit with only the relevant variable
regressor = lm(formula = Profit ~ R.D.Spend, data = train_df)
summary(regressor)
# Predicting the test results
y_pred = predict(regressor,newdata = test_df)
y_pred = predict(regressor,newdata = test_df)
y_pred
df
y_pred
test_df
# Visualizing the Test Set Result
ggplot() + geom_point(aes(x = test_df$R.D.Spend,y = test_df$Profit),color = 'red') +
geom_line(aes(x=test_df$R.D.Spend,y = predict(regressor,newdata = test_df)),color = 'blue') +
ggtitle('Profit vs Experience (Test Set)') + xlab('R.D.Spend') + ylab('Profit')
ggplot() + geom_point(aes(x = train_df$R.D.Spend,y = train_df$Profit),color = 'red') +
geom_line(aes(x=train_df$R.D.Spend,y = predict(regressor,newdata = train_df)),color = 'blue') +
ggtitle('Profit vs Experience (Training Set)') + xlab('R.D.Spend') + ylab('Profit')
# Visualizing the Test Set Result
ggplot() + geom_point(aes(x = test_df$R.D.Spend,y = test_df$Profit),color = 'red') +
geom_line(aes(x=test_df$R.D.Spend,y = predict(regressor,newdata = test_df)),color = 'blue') +
ggtitle('Profit vs Experience (Test Set)') + xlab('R.D.Spend') + ylab('Profit')
rownames(df)
colnames(df)
IV = colnames(df)[colnames(df)!=Profit]
IV = colnames(df)[colnames(df)!='Profit']
paste(IV,sep="+")
paste(IV,collapse="+")
paste(IV,collapse=" + ")
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = Profit ~ parse(text=paste(IV,collapse=" + ")), data = train_df)
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = Profit ~ paste(IV,collapse=" + "), data = train_df)
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = Profit ~ eval(parse(text=paste(IV,collapse=" + "))), data = train_df)
eval(parse(text=paste(IV,collapse=" + ")))
train_df[eval(parse(text=paste(IV,collapse=" + ")))]
"prOFIT" +"2"
"prOFIT" %+%"2"
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
summary(regressor)
regressor$effects
regressor$rank
regressor$terms
# Summarize the regression output
summary(regressor)
summary(regressor)
# Summarize the regression output
sumary = summary(regressor)
sumary$fstatistic
sumary$cov.unscaled
sumary$residuals
sumary$terms
sumary$aliased
sumary$sigma
sumary$r.squared
sumary$adj.r.squared
sumary$call
sumary$terms
sumary$residuals
sumary$cov.unscaled
sumary
sumary$coefficients
coef = sumary$coefficients
coef
coef[,4]
coef[-1,4]
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
df
df$State
levels(df$State)
levels(df$State)[-1]
df = dfold[-c("State")]
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
df = df_pre[-c("State")]
df
df = df_pre[-"State"]
df = df_pre[,-"State"]
df = df_pre[[-"State"]]
df = df_pre[[-c("State")]]
df_pre[["State"]]
df = df_pre[[colnames(df_pre)!="State"]]
colnames(df_pre)!="State"
df = df_pre[[which(colnames(df_pre)!="State")]]
df = df_pre[which(colnames(df_pre)!="State")]
which(colnames(df_pre)!="State")
df
lvl = levels(df_pre$State)[-1]
df_pre$State ==
levels(df_pre$State)[1]
for (l in lvl){
df[[paste0("State",l)]] = df_pre$State == l
}
df
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
IV = colnames(df)[colnames(df)!='Profit']
IV
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = df_pre$State == l
}
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
IV = colnames(df)[colnames(df)!='Profit']
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
Pvalue
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = (df_pre$State == l)*1
}
df
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
IV = colnames(df)[colnames(df)!='Profit']
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
Pvalue
colnames(Pvalue)[which.max(Pvalue)]
which.max(Pvalue)
colnames(Pvalue)
names(Pvalue)
names(Pvalue)[which.max(Pvalue)]
IV
IV[IV!=names(Pvalue)[which.max(Pvalue)]]
Pvalue = 1 # Initialization to get in the loop
while (max(Pvalue) > 0.05){
IV = IV[IV!=names(Pvalue)[which.max(Pvalue)]]
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
}
# Backward Elimination (remove highest P-value < 0.05)
#
# Multiple Linear regression, given X predict Y as Yhat.
# For non linear prediction one could use feature transformation on X or Y
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = (df_pre$State == l)*1
}
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
IV = colnames(df)[colnames(df)!='Profit']
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
while (max(Pvalue) > 0.05){
IV = IV[IV!=names(Pvalue)[which.max(Pvalue)]]
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste("Profit ~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
}
regressor
regressor
summary(regressor)
backwardElimination = function(df,p,y,IV){
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste(y,"~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
while (max(Pvalue) > p){
IV = IV[IV!=names(Pvalue)[which.max(Pvalue)]]
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste(y,"~",paste(IV,collapse=" + ")) )), data = train_df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
}
return(list(IV=IV,regressor=regressor))
}
# Backward Elimination (remove highest P-value < 0.05)
#
# Multiple Linear regression, given X predict Y as Yhat.
# For non linear prediction one could use feature transformation on X or Y
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = (df_pre$State == l)*1
}
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
IV = colnames(df)[colnames(df)!='Profit']
# Backward Elimination (remove highest P-value < 0.05)
#
# Multiple Linear regression, given X predict Y as Yhat.
# For non linear prediction one could use feature transformation on X or Y
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = (df_pre$State == l)*1
}
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
IV = colnames(df)[colnames(df)!='Profit']
# Backward Elimination (remove highest P-value < 0.05)
backwardElimination = function(df,p,y,IV){
# Fitting Simple Linear Regression to the Training Set
regressor = lm(formula = eval(parse( text= paste(y,"~",paste(IV,collapse=" + ")) )), data = df)
# Summarize the regression output
Pvalue = summary(regressor)$coefficients[-1,4]
while (max(Pvalue) > p){
IV = IV[IV!=names(Pvalue)[which.max(Pvalue)]]
regressor = lm(formula = eval(parse( text= paste(y,"~",paste(IV,collapse=" + ")) )), data = df)
Pvalue = summary(regressor)$coefficients[-1,4]
}
return(list(IV=IV,regressor=regressor))
}
#
# Multiple Linear regression, given X predict Y as Yhat.
# For non linear prediction one could use feature transformation on X or Y
# Goal : Minimize sum of square residual = sum( (Y_i - Yhat_i)^2 )
#
# Set working directory
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = (df_pre$State == l)*1
}
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
y = 'Profit'
IV = colnames(df)[colnames(df)!=y]
backwardElimination(train_df,0.05,y,IV)
list[IV,train_df] = backwardElimination(train_df,0.05,y,IV)
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 2 - Regression/Section 5 - Multiple Linear Regression/R")# Importing the dataset where string variable is treated as factor
df_pre = read.csv('50_Startups.csv',stringsAsFactors = TRUE)
# Preprocess categorical variable for backward Elimination
df = df_pre[which(colnames(df_pre)!="State")]
lvl = levels(df_pre$State)[-3]
for (l in lvl){
df[[paste0("State",l)]] = (df_pre$State == l)*1
}
# Splitting the dataset into (Training set) and (Test set)
library(caTools)
set.seed(123)
split = sample.split(df$Profit,SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
y = 'Profit'
IV = colnames(df)[colnames(df)!=y]
list[IV,regressor] = backwardElimination(train_df,0.05,y,IV)
library(gsubfn)
list[IV,regressor] = backwardElimination(train_df,0.05,y,IV)
list[IV,regressor] = backwardElimination(train_df,0.05,y,IV)
library(gsubfn)
list[IV,regressor] = backwardElimination(train_df,0.05,y,IV)
