# Feature Extraction (Dimensionality Reduction)
# Principal Component Analysis
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 9 - Dimensionality Reduction/Section 45 - Kernel PCA/R")
library(caTools) # For splitting test and train
df = read.csv('Social_Network_Ads.csv',stringsAsFactors = TRUE)
df = df[!(names(df) %in% c("User.ID","Gender"))] # remove user ID and gender
# define dependent variable (y) and independent variable (IV)
y = 'Purchased'
df[[y]] = factor(df[[y]]) # set DP as factor variable
IV = colnames(df)[colnames(df)!=y]
# Set seed
set.seed(123)
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.75)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
train_m = colMeans(train_df[IV])
train_sd = sapply(train_df[IV],sd)
unscale = function(df,m,sd){
n = ncol(df)
unscale_df = sapply(1:n,function(c) df[c] * sd[c] + m[c])
return(data.frame(unscale_df))
}
myscale = function(df,m,sd){
n = ncol(df)
scale_df = sapply(1:n,function(c) (df[c] - m[c])/sd[c])
return(data.frame(scale_df))
}
# # Feature Scaling (beneficial for visualization and normalization)
train_df[IV] = myscale(train_df[IV],train_m,train_sd)
test_df[IV] =  myscale(test_df[IV],train_m,train_sd)
# Applying Kernal PCA
library(kernlab)
kpca = kpca(~., data=train_df[IV],kernel = 'rbfdot',features = 2)
train_pca = data.frame(predict(pca,train_df))
train_pca = data.frame(predict(kpca,train_df))
train_pca = cbind(data.frame(predict(kpca,train_df)),train_df[y])
IV_pca = c("V1","V2")
test_pca = cbind(data.frame(predict(kpca,test_df)),train_df[y])
train_pca = cbind(data.frame(predict(kpca,train_df)),train_df[y])
train_pca
test_pca = cbind(data.frame(predict(kpca,test_df)),test_df[y])
test_pca
# Building Classifier
classifier = glm(formula = Purchased ~ . ,family = binomial, data = train_pca)
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_pca[IV_pca])
test_pca
head(test_pca)
test_pca[IV_pca]
IV_pca = c("X1","X2")
IV_pca = c("X1","X2")
test_pca[IV_pca]
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_pca[IV_pca])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
CM
cm
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_pca[IV_pca]) > 0.5
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
cm
# Making prediction on the test set
y_pred = 1*(predict(classifier,newdata = test_pca[IV_pca]) > 0.5)
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
cm
set = train_pca
X1 = seq(min(set["X1"]),max(set["X1"]),length.out = 150)
X2 = seq(min(set["X2"]),max(set["X2"]),length.out = 150)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = IV_pca
y_grid = predict(classifier,newdata = grid_set)
plot(grid_set,pch= '.',cex=3,col = ifelse(y_grid==1,'springgreen3','tomato' ),
main = 'Decission Tree Classifier (Train)',xlab = 'Age',
ylab = 'Estimated Salary',xlim = range(X1),ylim=range(X2))
points(set,pch= 21,bg = ifelse(set[[y]]==1,'green4','red3' ))
set[['X1']]
set[['X2']]
set = train_pca
X1 = seq(min(set['X1']),max(set['X1']),length.out = 150)
X2 = seq(min(set['X2']),max(set['X2']),length.out = 150)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = IV_pca
y_grid = predict(classifier,newdata = grid_set)
plot(grid_set,pch= '.',cex=3,col = ifelse(y_grid==1,'springgreen3','tomato' ),
main = 'Decission Tree Classifier (Train)',xlab = 'Age',
ylab = 'Estimated Salary',xlim = range(X1),ylim=range(X2))
points(set[IV_pca],pch= 21,bg = ifelse(set[[y]]==1,'green4','red3' ))
CM
cm
set = train_pca
X1 = seq(min(set['X1']),max(set['X1']),length.out = 150)
X2 = seq(min(set['X2']),max(set['X2']),length.out = 150)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = IV_pca
y_grid = 1*(predict(classifier,newdata = test_pca[IV_pca]) > 0.5)
plot(grid_set,pch= '.',cex=3,col = ifelse(y_grid==1,'springgreen3','tomato' ),
main = 'Decission Tree Classifier (Train)',xlab = 'Age',
ylab = 'Estimated Salary',xlim = range(X1),ylim=range(X2))
points(set[IV_pca],pch= 21,bg = ifelse(set[[y]]==1,'green4','red3' ))
set = train_pca
X1 = seq(min(set['X1']),max(set['X1']),length.out = 150)
X2 = seq(min(set['X2']),max(set['X2']),length.out = 150)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = IV_pca
y_grid = 1*(predict(classifier,newdata = grid_set) > 0.5)
plot(grid_set,pch= '.',cex=3,col = ifelse(y_grid==1,'springgreen3','tomato' ),
main = 'Decission Tree Classifier (Train)',xlab = 'Age',
ylab = 'Estimated Salary',xlim = range(X1),ylim=range(X2))
points(set[IV_pca],pch= 21,bg = ifelse(set[[y]]==1,'green4','red3' ))
plot(grid_set,pch= '.',cex=3,col = ifelse(y_grid==1,'springgreen3','tomato' ),
main = 'Decission Tree Classifier (Test)',xlab = 'Age',
ylab = 'Estimated Salary',xlim = range(X1),ylim=range(X2))
points(test_pca[IV_pca],pch= 21,bg = ifelse(test_df[[y]]==1,'green4','red3' ))
# Applying Kernal PCA
library(kernlab)
kpca = kpca(~., data=train_df[IV],kernel = 'splinedot',features = 2)
kpca = kpca(~., data=train_df[IV],kernel = 'tanhdot',features = 2)
kpca = kpca(~., data=train_df[IV],kernel = 'polydot',features = 2)
kpca = kpca(~., data=train_df[IV],kernel = 'rbfdot',features = 2)
pcv(kpca)
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 48 - Model Selection/R")
# K-fold CV
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 48 - Model Selection/R")
library(caTools) # For splitting test and train
# Set working directory
# Importing the dataset where string variable is treated as factor
df = read.csv('Social_Network_Ads.csv',stringsAsFactors = TRUE)
df = df[!(names(df) %in% c("User.ID","Gender"))] # remove user ID and gender
# define dependent variable (y) and independent variable (IV)
y = 'Purchased'
df[[y]] = factor(df[[y]]) # set DP as factor variable
IV = colnames(df)[colnames(df)!=y]
# Set seed
set.seed(123)
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.75)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
train_m = colMeans(train_df[IV])
train_sd = sapply(train_df[IV],sd)
unscale = function(df,m,sd){
n = ncol(df)
unscale_df = sapply(1:n,function(c) df[c] * sd[c] + m[c])
return(data.frame(unscale_df))
}
myscale = function(df,m,sd){
n = ncol(df)
scale_df = sapply(1:n,function(c) (df[c] - m[c])/sd[c])
return(data.frame(scale_df))
}
# Feature Scaling (beneficial for visualization and normalization)
train_df[IV] = myscale(train_df[IV],train_m,train_sd)
test_df[IV] =  myscale(test_df[IV],train_m,train_sd)
# Building Classifier
library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
# Applying k-fold Cross Validation
library(caret)
folds = createFolds(train_df[[y]],k=10)
cm
diag(cm)
sum(diag(cm))
sum(cm)
library(caret)
folds = createFolds(train_df[[y]],k=10)
cv = lapply(folds,function(f) {
train_fold = train_df[-f,]
test_fold = train_df[f,]
classifier = svm(formula = Purchased ~ .,data = train_fold,
type = 'C-classification',kernel='radial')
y_pred = predict(classifier,newdata = test_fold[IV])
cm = table(test_fold[[y]],y_pred)
accuracy = sum(diag(cm))/sum(cm)
return(accuracy)
})
cv
cv = sapply(folds,function(f) {
train_fold = train_df[-f,]
test_fold = train_df[f,]
classifier = svm(formula = Purchased ~ .,data = train_fold,
type = 'C-classification',kernel='radial')
y_pred = predict(classifier,newdata = test_fold[IV])
cm = table(test_fold[[y]],y_pred)
accuracy = sum(diag(cm))/sum(cm)
return(accuracy)
})
cv
which.max(cv)
accuracy = mean(cv)
accuracy
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 49 - XGBoost/R")
# K-fold CV
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 48 - Model Selection/R")
library(caTools) # For splitting test and train
# Set working directory
# Importing the dataset where string variable is treated as factor
df = read.csv('Social_Network_Ads.csv',stringsAsFactors = TRUE)
df = df[!(names(df) %in% c("User.ID","Gender"))] # remove user ID and gender
# define dependent variable (y) and independent variable (IV)
y = 'Purchased'
df[[y]] = factor(df[[y]]) # set DP as factor variable
IV = colnames(df)[colnames(df)!=y]
# Set seed
set.seed(123)
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.75)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
train_m = colMeans(train_df[IV])
train_sd = sapply(train_df[IV],sd)
unscale = function(df,m,sd){
n = ncol(df)
unscale_df = sapply(1:n,function(c) df[c] * sd[c] + m[c])
return(data.frame(unscale_df))
}
myscale = function(df,m,sd){
n = ncol(df)
scale_df = sapply(1:n,function(c) (df[c] - m[c])/sd[c])
return(data.frame(scale_df))
}
# Feature Scaling (beneficial for visualization and normalization)
train_df[IV] = myscale(train_df[IV],train_m,train_sd)
test_df[IV] =  myscale(test_df[IV],train_m,train_sd)
# K-fold CV
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 48 - Model Selection/R")
library(caTools) # For splitting test and train
# Set working directory
# Importing the dataset where string variable is treated as factor
df = read.csv('Social_Network_Ads.csv',stringsAsFactors = TRUE)
df = df[!(names(df) %in% c("User.ID","Gender"))] # remove user ID and gender
# define dependent variable (y) and independent variable (IV)
y = 'Purchased'
df[[y]] = factor(df[[y]]) # set DP as factor variable
IV = colnames(df)[colnames(df)!=y]
# Set seed
set.seed(123)
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.75)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
train_m = colMeans(train_df[IV])
train_sd = sapply(train_df[IV],sd)
unscale = function(df,m,sd){
n = ncol(df)
unscale_df = sapply(1:n,function(c) df[c] * sd[c] + m[c])
return(data.frame(unscale_df))
}
myscale = function(df,m,sd){
n = ncol(df)
scale_df = sapply(1:n,function(c) (df[c] - m[c])/sd[c])
return(data.frame(scale_df))
}
# Feature Scaling (beneficial for visualization and normalization)
train_df[IV] = myscale(train_df[IV],train_m,train_sd)
test_df[IV] =  myscale(test_df[IV],train_m,train_sd)
# Applying grid search
library(caret)
classifier = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
classifier
classifier$bestTune
classifier$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial',
sigma = classifier$bestTune[1], C = classifier$bestTune[2])
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
cm
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
cm
classifier$bestTune
classifier = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
classifier$bestTune
findparam$bestTune
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
# Making prediction on the test set
y_pred = predict(findparam,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_cm = table(test_df[[y]],opt_y_pred)
opt_y_pred = predict(opt_class,newdata = test_df[IV])
opt_cm = table(test_df[[y]],opt_y_pred)
opt_cm
data
opt_y_pred = predict(opt_class,newdata = test_df[IV])
table(test_df[[y]],opt_y_pred)
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
y_pred = predict(findparam,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
cm
findparam$bestTune
library(caret)
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
findparam = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
findparam$bestTune
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_class$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_class$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_class$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_class$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_class$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
opt_class = train(form = Purchased ~ . ,data = train_df, method = 'svmRadial')
opt_class$bestTune
# # Building Classifier
# library(e1071)
classifier = svm(formula = Purchased ~ .,data = train_df,
type = 'C-classification',kernel='radial')
# Making prediction on the test set
y_pred = predict(classifier,newdata = test_df[IV])
opt_y_pred = predict(opt_class,newdata = test_df[IV])
# Confusion Matrix
cm = table(test_df[[y]],y_pred)
opt_cm = table(test_df[[y]],opt_y_pred)
cm
opt_cm
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 49 - XGBoost/R")
# XGBoost
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 49 - XGBoost/R")
# Import the dataset
df = read.csv('Churn_Modelling.csv')
View(df)
# Import the dataset
df = read.csv('Churn_Modelling.csv')
df = df[4:14]
# Import the dataset
df = read.csv('Churn_Modelling.csv')
names(df)
V = names(df)[!names(df) %in% c("RowNumber", "CustomerId" ,"Surname")]
df = df[V]
# XGBoost
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 49 - XGBoost/R")
# Import the dataset
df = read.csv('Churn_Modelling.csv',stringsAsFactors = TRUE)
V = names(df)[!names(df) %in% c("RowNumber", "CustomerId" ,"Surname")]
df = df[V]
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.75)
V
IV = V[V!=y]
df = df[V]
y = "Exited"
IV = V[V!=y]
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.75)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 49 - XGBoost/R")
# Import the dataset
df = read.csv('Churn_Modelling.csv',stringsAsFactors = TRUE)
V = names(df)[!names(df) %in% c("RowNumber", "CustomerId" ,"Surname")]
df = df[V]
y = "Exited"
IV = V[V!=y]
# Splitting the dataset into (Training set) and (Test set)
split = sample.split(df[[y]],SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
# Splitting the dataset into (Training set) and (Test set)
set.seed(123)
split = sample.split(df[[y]],SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
# Fitting XGboost to the Training Set
library(xgboost)
# Fitting XGboost to the Training Set
library(xgboost)
train_df[IV]
matrix(train_df[IV])
ax.matrix(train_df[IV])
as.matrix(train_df[IV])
classifier = xgboost(data = as.matrix(train_df[IV]), label = train_df[[y]],nround=10)
as.data.frame(lapply(df, as.numeric))
# XGBoost
setwd("C:/GITHUB/Machine-Learning-A-Z-Codes-and-Datasets-/Part 10 - Model Selection and Boosting/Section 49 - XGBoost/R")
# Import the dataset
df = read.csv('Churn_Modelling.csv',stringsAsFactors = TRUE)
df = as.data.frame(lapply(df, as.numeric))
V = names(df)[!names(df) %in% c("RowNumber", "CustomerId" ,"Surname")]
df = df[V]
y = "Exited"
IV = V[V!=y]
# Splitting the dataset into (Training set) and (Test set)
set.seed(123)
split = sample.split(df[[y]],SplitRatio = 0.8)
train_df = subset(df,split == TRUE)
test_df = subset(df,split == FALSE)
# Fitting XGboost to the Training Set
library(xgboost)
classifier = xgboost(data = as.matrix(train_df[IV]), label = train_df[[y]],nround=10)
folds = createFolds(train_df[[y]],k=10)
cv = sapply(folds,function(f) {
train_fold = train_df[-f,]
test_fold = train_df[f,]
classifier = xgboost(data = as.matrix(train_fold[IV]), label = train_fold[[y]],nround=10)
y_pred = predict(classifier,newdata = as.matrix(test_fold[IV])) >= 0.5
cm = table(test_fold[[y]],y_pred)
accuracy = sum(diag(cm))/sum(cm)
return(accuracy)
})
accuracy = mean(cv)
accuracy
